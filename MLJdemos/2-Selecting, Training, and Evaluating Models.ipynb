{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Fisher Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\textbf{Author}: R.A. Fisher   \\textbf{Source}: \\href{https://archive.ics.uci.edu/ml/datasets/Iris}{UCI} - 1936 - Donated by Michael Marshall   \\textbf{Please cite}:   \n",
       "\n",
       "\\textbf{Iris Plants Database}   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda \\& Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "\\subsubsection{Attribute Information:}\n",
       "\\begin{verbatim}\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "**Author**: R.A. Fisher   **Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall   **Please cite**:   \n",
       "\n",
       "**Iris Plants Database**   This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "Predicted attribute: class of iris plant.   This is an exceedingly simple domain.  \n",
       "\n",
       "### Attribute Information:\n",
       "\n",
       "```\n",
       "1. sepal length in cm\n",
       "2. sepal width in cm\n",
       "3. petal length in cm\n",
       "4. petal width in cm\n",
       "5. class: \n",
       "   -- Iris Setosa\n",
       "   -- Iris Versicolour\n",
       "   -- Iris Virginica\n",
       "```\n"
      ],
      "text/plain": [
       "  \u001b[1mAuthor\u001b[22m: R.A. Fisher \u001b[1mSource\u001b[22m: UCI\n",
       "  (https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael\n",
       "  Marshall \u001b[1mPlease cite\u001b[22m:\n",
       "\n",
       "  \u001b[1mIris Plants Database\u001b[22m This is perhaps the best known database to be found in\n",
       "  the pattern recognition literature. Fisher's paper is a classic in the field\n",
       "  and is referenced frequently to this day. (See Duda & Hart, for example.)\n",
       "  The data set contains 3 classes of 50 instances each, where each class\n",
       "  refers to a type of iris plant. One class is linearly separable from the\n",
       "  other 2; the latter are NOT linearly separable from each other.\n",
       "\n",
       "  Predicted attribute: class of iris plant. This is an exceedingly simple\n",
       "  domain.\n",
       "\n",
       "\u001b[1m  Attribute Information:\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[36m  1. sepal length in cm\u001b[39m\n",
       "\u001b[36m  2. sepal width in cm\u001b[39m\n",
       "\u001b[36m  3. petal length in cm\u001b[39m\n",
       "\u001b[36m  4. petal width in cm\u001b[39m\n",
       "\u001b[36m  5. class: \u001b[39m\n",
       "\u001b[36m     -- Iris Setosa\u001b[39m\n",
       "\u001b[36m     -- Iris Versicolour\u001b[39m\n",
       "\u001b[36m     -- Iris Virginica\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OpenML.describe_dataset(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>4 rows × 5 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>sepallength</th><th>sepalwidth</th><th>petallength</th><th>petalwidth</th><th>class</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"CategoricalArrays.CategoricalValue{String, UInt32}\">Cat…</th></tr></thead><tbody><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& sepallength & sepalwidth & petallength & petalwidth & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sepallength \u001b[0m\u001b[1m sepalwidth \u001b[0m\u001b[1m petallength \u001b[0m\u001b[1m petalwidth \u001b[0m\u001b[1m class       \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Cat…        \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────\n",
       "   1 │         5.1         3.5          1.4         0.2  Iris-setosa\n",
       "   2 │         4.9         3.0          1.4         0.2  Iris-setosa\n",
       "   3 │         4.7         3.2          1.3         0.2  Iris-setosa\n",
       "   4 │         4.6         3.1          1.5         0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = OpenML.load(61);\n",
    "iris = DataFrame(iris); \n",
    "first(iris, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Build a model to predcit the `:class` variable given the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬──────────────────────────────────┬───────────────┐\n",
       "│\u001b[22m _.names     \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n",
       "├─────────────┼──────────────────────────────────┼───────────────┤\n",
       "│ sepallength │ Float64                          │ Continuous    │\n",
       "│ sepalwidth  │ Float64                          │ Continuous    │\n",
       "│ petallength │ Float64                          │ Continuous    │\n",
       "│ petalwidth  │ Float64                          │ Continuous    │\n",
       "│ class       │ CategoricalValue{String, UInt32} │ Multiclass{3} │\n",
       "└─────────────┴──────────────────────────────────┴───────────────┘\n",
       "_.nrows = 150\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬──────────────────────────────────┬───────────────┐\n",
       "│\u001b[22m _.names     \u001b[0m│\u001b[22m _.types                          \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n",
       "├─────────────┼──────────────────────────────────┼───────────────┤\n",
       "│ sepallength │ Float64                          │ Continuous    │\n",
       "│ sepalwidth  │ Float64                          │ Continuous    │\n",
       "│ petallength │ Float64                          │ Continuous    │\n",
       "│ petalwidth  │ Float64                          │ Continuous    │\n",
       "│ class       │ CategoricalValue{String, UInt32} │ Multiclass{3} │\n",
       "└─────────────┴──────────────────────────────────┴───────────────┘\n",
       "_.nrows = 150\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of any missing values\n",
    "coerce!(iris,\n",
    "        Union{Missing,Continuous}=>Continuous,\n",
    "        Union{Missing,Multiclass}=>Multiclass,\n",
    "        tight=true)\n",
    "schema(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into input and target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(iris, ==(:class), name->true, rng=42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractVector{Multiclass{3}} (alias for AbstractArray{Multiclass{3}, 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scitype(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighborModels, ... )\n",
       " (name = KernelPerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = MultinomialNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = PegasosClassifier, package_name = BetaML, ... )\n",
       " (name = PerceptronClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_julia_classifiers(meta) = AbstractVector{Finite} <: meta.target_scitype && meta.is_pure_julia\n",
    "\n",
    "models(filter_julia_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all models with \"Classifier\" in their name or docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BernoulliNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ComplementNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " ⋮\n",
       " (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(\"Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, let's find all models that match our data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " ⋮\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and instantiate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffRules ─ v1.8.0\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/gitRepos/ml-demos/Project.toml`\n",
      " \u001b[90m [094fc8d1] \u001b[39m\u001b[92m+ MLJFlux v0.2.5\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/gitRepos/ml-demos/Manifest.toml`\n",
      " \u001b[90m [621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.0.1\u001b[39m\n",
      " \u001b[90m [1520ce14] \u001b[39m\u001b[92m+ AbstractTrees v0.3.4\u001b[39m\n",
      " \u001b[90m [4fba245c] \u001b[39m\u001b[92m+ ArrayInterface v3.2.1\u001b[39m\n",
      " \u001b[90m [ab4f0b2a] \u001b[39m\u001b[92m+ BFloat16s v0.2.0\u001b[39m\n",
      " \u001b[90m [fa961155] \u001b[39m\u001b[92m+ CEnum v0.4.1\u001b[39m\n",
      " \u001b[90m [052768ef] \u001b[39m\u001b[92m+ CUDA v3.5.0\u001b[39m\n",
      " \u001b[90m [082447d4] \u001b[39m\u001b[92m+ ChainRules v1.16.0\u001b[39m\n",
      " \u001b[90m [bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.0\u001b[39m\n",
      " \u001b[90m [163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.0.3\u001b[39m\n",
      " \u001b[90m [b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.8.0\u001b[39m\n",
      " \u001b[90m [e2ba6199] \u001b[39m\u001b[92m+ ExprTools v0.1.6\u001b[39m\n",
      " \u001b[90m [587475ba] \u001b[39m\u001b[92m+ Flux v0.12.8\u001b[39m\n",
      " \u001b[90m [f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v0.10.23\u001b[39m\n",
      " \u001b[90m [d9f16b24] \u001b[39m\u001b[92m+ Functors v0.2.7\u001b[39m\n",
      " \u001b[90m [0c68f7d7] \u001b[39m\u001b[92m+ GPUArrays v8.1.2\u001b[39m\n",
      " \u001b[90m [61eb1bfa] \u001b[39m\u001b[92m+ GPUCompiler v0.13.9\u001b[39m\n",
      " \u001b[90m [7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.4\u001b[39m\n",
      " \u001b[90m [615f187c] \u001b[39m\u001b[92m+ IfElse v0.1.1\u001b[39m\n",
      " \u001b[90m [e5e0dc1b] \u001b[39m\u001b[92m+ Juno v0.8.4\u001b[39m\n",
      " \u001b[90m [929cbde3] \u001b[39m\u001b[92m+ LLVM v4.7.0\u001b[39m\n",
      " \u001b[90m [094fc8d1] \u001b[39m\u001b[92m+ MLJFlux v0.2.5\u001b[39m\n",
      " \u001b[90m [e89f7d12] \u001b[39m\u001b[92m+ Media v0.5.0\u001b[39m\n",
      " \u001b[90m [872c559c] \u001b[39m\u001b[92m+ NNlib v0.7.31\u001b[39m\n",
      " \u001b[90m [a00861dc] \u001b[39m\u001b[92m+ NNlibCUDA v0.1.11\u001b[39m\n",
      " \u001b[90m [74087812] \u001b[39m\u001b[92m+ Random123 v1.4.2\u001b[39m\n",
      " \u001b[90m [e6cf234a] \u001b[39m\u001b[92m+ RandomNumbers v1.5.3\u001b[39m\n",
      " \u001b[90m [c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n",
      " \u001b[90m [aedffcd0] \u001b[39m\u001b[92m+ Static v0.4.0\u001b[39m\n",
      " \u001b[90m [a759f4b9] \u001b[39m\u001b[92m+ TimerOutputs v0.5.13\u001b[39m\n",
      " \u001b[90m [a5390f91] \u001b[39m\u001b[92m+ ZipFile v0.9.4\u001b[39m\n",
      " \u001b[90m [e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.32\u001b[39m\n",
      " \u001b[90m [700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.2\u001b[39m\n",
      " \u001b[90m [dad2f222] \u001b[39m\u001b[92m+ LLVMExtra_jll v0.0.13+0\u001b[39m\n",
      " \u001b[90m [4af54fe1] \u001b[39m\u001b[92m+ LazyArtifacts\u001b[39m\n",
      " \u001b[90m [9abbd945] \u001b[39m\u001b[92m+ Profile\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygote\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFlux\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJFlux\n",
      "  5 dependencies successfully precompiled in 23 seconds (211 already precompiled)\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"MLJFlux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJFlux ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /home/john/.julia/packages/MLJModels/GKDnU/src/loading.jl:168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJFlux.NeuralNetworkClassifier"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a model\n",
    "\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkClassifier(\n",
       "    builder = Short(\n",
       "            n_hidden = 0,\n",
       "            dropout = 0.5,\n",
       "            σ = NNlib.σ),\n",
       "    finaliser = NNlib.softmax,\n",
       "    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),\n",
       "    loss = Flux.Losses.crossentropy,\n",
       "    epochs = 10,\n",
       "    batch_size = 1,\n",
       "    lambda = 0.0,\n",
       "    alpha = 0.0,\n",
       "    rng = Random._GLOBAL_RNG(),\n",
       "    optimiser_changes_trigger_retraining = false,\n",
       "    acceleration = CPU1{Nothing}(nothing))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model \n",
    "model = NeuralNetworkClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[35mA neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \u001b[39m\n",
       "\u001b[35m→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\u001b[39m\n",
       "\u001b[35m→ do `@load NeuralNetworkClassifier pkg=\"MLJFlux\"` to use the model.\u001b[39m\n",
       "\u001b[35m→ do `?NeuralNetworkClassifier` for documentation.\u001b[39m\n",
       "(name = \"NeuralNetworkClassifier\",\n",
       " package_name = \"MLJFlux\",\n",
       " is_supervised = true,\n",
       " abstract_type = Probabilistic,\n",
       " deep_properties = (:optimiser, :builder),\n",
       " docstring = \"A neural network model for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. \\n→ based on [MLJFlux](https://github.com/alan-turing-institute/MLJFlux.jl).\\n→ do `@load NeuralNetworkClassifier pkg=\\\"MLJFlux\\\"` to use the model.\\n→ do `?NeuralNetworkClassifier` for documentation.\",\n",
       " fit_data_scitype = Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{<:Finite}},\n",
       " hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing),\n",
       " hyperparameter_types = (\"MLJFlux.Short\", \"typeof(NNlib.softmax)\", \"Flux.Optimise.ADAM\", \"typeof(Flux.Losses.crossentropy)\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\"),\n",
       " hyperparameters = (:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration),\n",
       " implemented_methods = Any[],\n",
       " inverse_transform_scitype = Unknown,\n",
       " is_pure_julia = true,\n",
       " is_wrapper = false,\n",
       " iteration_parameter = :epochs,\n",
       " load_path = \"MLJFlux.NeuralNetworkClassifier\",\n",
       " package_license = \"MIT\",\n",
       " package_url = \"https://github.com/alan-turing-institute/MLJFlux.jl\",\n",
       " package_uuid = \"094fc8d1-fd35-5302-93ea-dabda2abf845\",\n",
       " predict_scitype = AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:Finite},\n",
       " prediction_type = :probabilistic,\n",
       " supports_class_weights = false,\n",
       " supports_online = false,\n",
       " supports_training_losses = true,\n",
       " supports_weights = false,\n",
       " transform_scitype = Unknown,\n",
       " input_scitype = Table{<:AbstractVector{<:Continuous}},\n",
       " target_scitype = AbstractVector{<:Finite},\n",
       " output_scitype = Unknown,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLJ model is just a mutable struct holding hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting, predicting, and inspecting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in MLJ, model training/validation are bound together in a `machine()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 0 times; caches data\n",
       "  args: \n",
       "    1:\tSource @309 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @384 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `machine` stores the *learned* parameters. \n",
    "\n",
    "Let's train our machine on 70% of the data and evaluate on a holdout set with 30%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([84, 30, 106, 7, 133, 114, 134, 42, 23, 136  …  59, 78, 98, 104, 9, 29, 119, 13, 20, 12], [3, 83, 56, 95, 34, 41, 127, 87, 25, 66  …  38, 61, 122, 27, 39, 22, 123, 113, 146, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = partition(eachindex(y), 0.7, shuffle=true) # produce indices for train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can `fit!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:403\n",
      "┌ Info: Loss is 1.644\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.56\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.42\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.254\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.235\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.192\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.095\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.152\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.122\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.154\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.137\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.126\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n",
       "  args: \n",
       "    1:\tSource @309 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @384 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can `predict` via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element MLJBase.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.288, Iris-versicolor=>0.341, Iris-virginica=>0.371)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.287, Iris-versicolor=>0.343, Iris-virginica=>0.37)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.292, Iris-versicolor=>0.32, Iris-virginica=>0.388)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach, rows=test)\n",
    "ŷ[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element MLJBase.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.288, Iris-versicolor=>0.341, Iris-virginica=>0.371)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.287, Iris-versicolor=>0.343, Iris-virginica=>0.37)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.292, Iris-versicolor=>0.32, Iris-virginica=>0.388)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach, X[test, :])\n",
    "ŷ[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that our machine is fitted, we can inspect the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chain = Chain(Chain(Dense(4, 3, σ), Dropout(0.5), Dense(3, 3)), softmax),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(mach)  # see which params were fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(training_losses = [1.4336826726115968, 1.6440542862286258, 1.5598450015697154, 1.420427569690476, 1.2537524057177758, 1.2348144041287794, 1.1916801438063866, 1.0948763132600585, 1.1517034759247204, 1.1215142027848028, 1.1543581231737337, 1.1367435105804533, 1.1261994034873632],)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(mach)  # print per epoch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a model via "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"neural_net.jlso\", mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a model via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 1 time; caches data\n",
       "  args: \n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach2 = machine(\"neural_net.jlso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now use our loaded model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element MLJBase.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.291, Iris-versicolor=>0.322, Iris-virginica=>0.387)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.291, Iris-versicolor=>0.328, Iris-virginica=>0.381)\n",
       " UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.288, Iris-versicolor=>0.341, Iris-virginica=>0.371)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = predict(mach2, X)\n",
    "ŷ[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to continue fitting a saved model, you must bind some data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:403\n",
      "\u001b[33mOptimising neural net:100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n",
       "  args: \n",
       "    1:\tSource @889 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @686 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach3 = machine(\"neural_net.jlso\", X, y)\n",
    "fit!(mach3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can perform a warm-restart (i.e. continue where we left off) by increasing the iteration parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs = model.epochs + 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:404\n",
      "┌ Info: Loss is 1.116\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.114\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.137\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.113\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 2 times; caches data\n",
       "  args: \n",
       "    1:\tSource @309 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @384 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a NN we can increase the `:learning_rate` without triggering a full restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:404\n",
      "┌ Info: Loss is 1.096\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.118\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.104\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.069\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 3 times; caches data\n",
       "  args: \n",
       "    1:\tSource @309 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @384 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs = model.epochs + 4 \n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changing any other hyperparameter will cause it to retrain from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:404\n",
      "┌ Info: Loss is 1.243\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 1.016\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.8939\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.936\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.8384\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7969\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7961\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.8009\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7878\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7648\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.8029\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7666\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.8263\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7552\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.6918\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7072\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.7962\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.72\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.8046\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n",
      "┌ Info: Loss is 0.6874\n",
      "└ @ MLJFlux /home/john/.julia/packages/MLJFlux/5ENQA/src/core.jl:128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 4 times; caches data\n",
       "  args: \n",
       "    1:\tSource @309 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @384 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training for a while longer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:404\n",
      "\u001b[33mOptimising neural net:100%[=========================] Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00967, Iris-versicolor=>0.352, Iris-virginica=>0.639)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":probabilistic"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(model).prediction_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have many options for probabilistic models. Let's try some out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnivariateFinite{Multiclass{3}}(Iris-setosa=>0.00967, Iris-versicolor=>0.352, Iris-virginica=>0.639)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.638565438150432"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the most likely value via `mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalArrays.CategoricalValue{String, UInt32} \"Iris-virginica\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode(yhat[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can broadcast these operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element Vector{Float64}:\n",
       " 0.638565438150432\n",
       " 0.026398091009350293\n",
       " 0.4731448025661525\n",
       " 0.028661296592622225\n",
       " 0.02387264457526202\n",
       " 0.025578062012357478\n",
       " 0.027718761407639916\n",
       " 0.7775105564941073\n",
       " 0.7028058978446596\n",
       " 0.2193457062515473\n",
       " 0.6711350744581164\n",
       " 0.02549231351613363\n",
       " 0.216710803640492\n",
       " ⋮\n",
       " 0.2862547703636074\n",
       " 0.2521823151120664\n",
       " 0.2447678292213899\n",
       " 0.2854750658679641\n",
       " 0.03545116929438963\n",
       " 0.20538838233744883\n",
       " 0.33173441741312554\n",
       " 0.24000902178038022\n",
       " 0.3022091532324718\n",
       " 0.5676189928229167\n",
       " 0.23074589352974542\n",
       " 0.025716434070151593"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.(yhat, \"Iris-virginica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n",
       " \"Iris-virginica\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-versicolor\"\n",
       " ⋮\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-virginica\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-setosa\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode.(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `predict_mode()` will give the predicted class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n",
       " \"Iris-virginica\"\n",
       " \"Iris-setosa\"\n",
       " \"Iris-versicolor\"\n",
       " \"Iris-setosa\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_mode(mach, X[test,:])[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ has a handy routine to do the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────────┬─────────────┬───────────┬──────────┐\n",
       "│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold \u001b[0m│\n",
       "├────────────────────────────┼─────────────┼───────────┼──────────┤\n",
       "│ LogLoss(tol = 2.22045e-16) │ 0.399       │ predict   │ [0.399]  │\n",
       "│ BrierScore()               │ -0.207      │ predict   │ [-0.207] │\n",
       "└────────────────────────────┴─────────────┴───────────┴──────────┘\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "    measures=[cross_entropy, brier_score]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can apply cross-validation instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:16\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n",
       "│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n",
       "├────────────────────────────┼─────────────┼───────────┼────────────────────────\n",
       "│ LogLoss(tol = 2.22045e-16) │ 0.415       │ predict   │ [0.31, 0.452, 0.467,  ⋯\n",
       "│ BrierScore()               │ -0.227      │ predict   │ [-0.165, -0.251, -0.2 ⋯\n",
       "└────────────────────────────┴─────────────┴───────────┴────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "    measures=[cross_entropy, brier_score]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use Monte Carlo cross-validation (e.g. with repeated randomized folds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 18 folds: 100%[=========================] Time: 0:00:46\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n",
       "│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n",
       "├────────────────────────────┼─────────────┼───────────┼────────────────────────\n",
       "│ LogLoss(tol = 2.22045e-16) │ 0.42        │ predict   │ [0.504, 0.43, 0.326,  ⋯\n",
       "│ BrierScore()               │ -0.237      │ predict   │ [-0.308, -0.232, -0.1 ⋯\n",
       "└────────────────────────────┴─────────────┴───────────┴────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "              repeats=3,\n",
    "              measures=[cross_entropy, brier_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`e` now has a number of properties we can inspect including `measure`, `measurement`, `per_fold`, and `per_observation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss= 0.41986513758286786 ± 0.06959214904530305\n",
      "brierscore= -0.23687596670543856 ± 0.04708818105209173\n"
     ]
    }
   ],
   "source": [
    "logloss = e.per_fold[1]\n",
    "println(\"logloss= \", mean(logloss), \" ± \", std(logloss))\n",
    "brier = e.per_fold[2]\n",
    "println(\"brierscore= \", mean(brier), \" ± \", std(brier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also restrict `evaluate()` to only the training rows if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Creating subsamples from a subset of all rows. \n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/resampling.jl:490\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:11\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n",
       "│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n",
       "├────────────────────────────┼─────────────┼───────────┼────────────────────────\n",
       "│ LogLoss(tol = 2.22045e-16) │ 0.414       │ predict   │ [0.375, 0.347, 0.462, ⋯\n",
       "│ BrierScore()               │ -0.231      │ predict   │ [-0.201, -0.185, -0.2 ⋯\n",
       "└────────────────────────────┴─────────────┴───────────┴────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{NeuralNetworkClassifier{Short,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:403\n",
      "\u001b[33mOptimising neural net:100%[=========================] Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Machine{NeuralNetworkClassifier{Short,…},…} trained 7 times; caches data\n",
       "  args: \n",
       "    1:\tSource @458 ⏎ `Table{AbstractVector{Continuous}}`\n",
       "    2:\tSource @955 ⏎ `AbstractVector{Multiclass{3}}`\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(mach, rows=train)    # re-train using all of `train` observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(mach, rows=test); # and predict missing targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip300\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip300)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip301\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip300)\" d=\"\n",
       "M156.274 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.274 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip302\">\n",
       "    <rect x=\"156\" y=\"47\" width=\"2197\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  176.996,1486.45 176.996,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  591.426,1486.45 591.426,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1005.86,1486.45 1005.86,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1420.29,1486.45 1420.29,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1834.72,1486.45 1834.72,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2249.15,1486.45 2249.15,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  176.996,1486.45 176.996,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  591.426,1486.45 591.426,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1005.86,1486.45 1005.86,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1420.29,1486.45 1420.29,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1834.72,1486.45 1834.72,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2249.15,1486.45 2249.15,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip300)\" d=\"M176.996 1517.37 Q173.385 1517.37 171.556 1520.93 Q169.75 1524.47 169.75 1531.6 Q169.75 1538.71 171.556 1542.27 Q173.385 1545.82 176.996 1545.82 Q180.63 1545.82 182.435 1542.27 Q184.264 1538.71 184.264 1531.6 Q184.264 1524.47 182.435 1520.93 Q180.63 1517.37 176.996 1517.37 M176.996 1513.66 Q182.806 1513.66 185.861 1518.27 Q188.94 1522.85 188.94 1531.6 Q188.94 1540.33 185.861 1544.94 Q182.806 1549.52 176.996 1549.52 Q171.186 1549.52 168.107 1544.94 Q165.051 1540.33 165.051 1531.6 Q165.051 1522.85 168.107 1518.27 Q171.186 1513.66 176.996 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M566.114 1544.91 L573.753 1544.91 L573.753 1518.55 L565.443 1520.21 L565.443 1515.95 L573.706 1514.29 L578.382 1514.29 L578.382 1544.91 L586.021 1544.91 L586.021 1548.85 L566.114 1548.85 L566.114 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M605.465 1517.37 Q601.854 1517.37 600.026 1520.93 Q598.22 1524.47 598.22 1531.6 Q598.22 1538.71 600.026 1542.27 Q601.854 1545.82 605.465 1545.82 Q609.1 1545.82 610.905 1542.27 Q612.734 1538.71 612.734 1531.6 Q612.734 1524.47 610.905 1520.93 Q609.1 1517.37 605.465 1517.37 M605.465 1513.66 Q611.276 1513.66 614.331 1518.27 Q617.41 1522.85 617.41 1531.6 Q617.41 1540.33 614.331 1544.94 Q611.276 1549.52 605.465 1549.52 Q599.655 1549.52 596.577 1544.94 Q593.521 1540.33 593.521 1531.6 Q593.521 1522.85 596.577 1518.27 Q599.655 1513.66 605.465 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M984.63 1544.91 L1000.95 1544.91 L1000.95 1548.85 L979.005 1548.85 L979.005 1544.91 Q981.667 1542.16 986.25 1537.53 Q990.857 1532.88 992.037 1531.53 Q994.283 1529.01 995.162 1527.27 Q996.065 1525.51 996.065 1523.82 Q996.065 1521.07 994.121 1519.33 Q992.199 1517.6 989.098 1517.6 Q986.898 1517.6 984.445 1518.36 Q982.014 1519.13 979.236 1520.68 L979.236 1515.95 Q982.061 1514.82 984.514 1514.24 Q986.968 1513.66 989.005 1513.66 Q994.375 1513.66 997.57 1516.35 Q1000.76 1519.03 1000.76 1523.52 Q1000.76 1525.65 999.954 1527.57 Q999.167 1529.47 997.06 1532.07 Q996.482 1532.74 993.38 1535.95 Q990.278 1539.15 984.63 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M1020.76 1517.37 Q1017.15 1517.37 1015.32 1520.93 Q1013.52 1524.47 1013.52 1531.6 Q1013.52 1538.71 1015.32 1542.27 Q1017.15 1545.82 1020.76 1545.82 Q1024.4 1545.82 1026.2 1542.27 Q1028.03 1538.71 1028.03 1531.6 Q1028.03 1524.47 1026.2 1520.93 Q1024.4 1517.37 1020.76 1517.37 M1020.76 1513.66 Q1026.57 1513.66 1029.63 1518.27 Q1032.71 1522.85 1032.71 1531.6 Q1032.71 1540.33 1029.63 1544.94 Q1026.57 1549.52 1020.76 1549.52 Q1014.95 1549.52 1011.88 1544.94 Q1008.82 1540.33 1008.82 1531.6 Q1008.82 1522.85 1011.88 1518.27 Q1014.95 1513.66 1020.76 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M1409.13 1530.21 Q1412.49 1530.93 1414.36 1533.2 Q1416.26 1535.47 1416.26 1538.8 Q1416.26 1543.92 1412.74 1546.72 Q1409.22 1549.52 1402.74 1549.52 Q1400.57 1549.52 1398.25 1549.08 Q1395.96 1548.66 1393.5 1547.81 L1393.5 1543.29 Q1395.45 1544.43 1397.76 1545.01 Q1400.08 1545.58 1402.6 1545.58 Q1407 1545.58 1409.29 1543.85 Q1411.61 1542.11 1411.61 1538.8 Q1411.61 1535.75 1409.45 1534.03 Q1407.32 1532.3 1403.5 1532.3 L1399.48 1532.3 L1399.48 1528.45 L1403.69 1528.45 Q1407.14 1528.45 1408.97 1527.09 Q1410.8 1525.7 1410.8 1523.11 Q1410.8 1520.45 1408.9 1519.03 Q1407.02 1517.6 1403.5 1517.6 Q1401.58 1517.6 1399.38 1518.01 Q1397.19 1518.43 1394.55 1519.31 L1394.55 1515.14 Q1397.21 1514.4 1399.52 1514.03 Q1401.86 1513.66 1403.92 1513.66 Q1409.25 1513.66 1412.35 1516.09 Q1415.45 1518.5 1415.45 1522.62 Q1415.45 1525.49 1413.81 1527.48 Q1412.16 1529.45 1409.13 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M1435.13 1517.37 Q1431.51 1517.37 1429.69 1520.93 Q1427.88 1524.47 1427.88 1531.6 Q1427.88 1538.71 1429.69 1542.27 Q1431.51 1545.82 1435.13 1545.82 Q1438.76 1545.82 1440.56 1542.27 Q1442.39 1538.71 1442.39 1531.6 Q1442.39 1524.47 1440.56 1520.93 Q1438.76 1517.37 1435.13 1517.37 M1435.13 1513.66 Q1440.94 1513.66 1443.99 1518.27 Q1447.07 1522.85 1447.07 1531.6 Q1447.07 1540.33 1443.99 1544.94 Q1440.94 1549.52 1435.13 1549.52 Q1429.31 1549.52 1426.24 1544.94 Q1423.18 1540.33 1423.18 1531.6 Q1423.18 1522.85 1426.24 1518.27 Q1429.31 1513.66 1435.13 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M1822.89 1518.36 L1811.08 1536.81 L1822.89 1536.81 L1822.89 1518.36 M1821.66 1514.29 L1827.54 1514.29 L1827.54 1536.81 L1832.47 1536.81 L1832.47 1540.7 L1827.54 1540.7 L1827.54 1548.85 L1822.89 1548.85 L1822.89 1540.7 L1807.29 1540.7 L1807.29 1536.19 L1821.66 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M1850.2 1517.37 Q1846.59 1517.37 1844.76 1520.93 Q1842.96 1524.47 1842.96 1531.6 Q1842.96 1538.71 1844.76 1542.27 Q1846.59 1545.82 1850.2 1545.82 Q1853.84 1545.82 1855.64 1542.27 Q1857.47 1538.71 1857.47 1531.6 Q1857.47 1524.47 1855.64 1520.93 Q1853.84 1517.37 1850.2 1517.37 M1850.2 1513.66 Q1856.01 1513.66 1859.07 1518.27 Q1862.15 1522.85 1862.15 1531.6 Q1862.15 1540.33 1859.07 1544.94 Q1856.01 1549.52 1850.2 1549.52 Q1844.39 1549.52 1841.31 1544.94 Q1838.26 1540.33 1838.26 1531.6 Q1838.26 1522.85 1841.31 1518.27 Q1844.39 1513.66 1850.2 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M2223.85 1514.29 L2242.2 1514.29 L2242.2 1518.22 L2228.13 1518.22 L2228.13 1526.7 Q2229.15 1526.35 2230.17 1526.19 Q2231.19 1526 2232.2 1526 Q2237.99 1526 2241.37 1529.17 Q2244.75 1532.34 2244.75 1537.76 Q2244.75 1543.34 2241.28 1546.44 Q2237.81 1549.52 2231.49 1549.52 Q2229.31 1549.52 2227.04 1549.15 Q2224.8 1548.78 2222.39 1548.04 L2222.39 1543.34 Q2224.47 1544.47 2226.69 1545.03 Q2228.92 1545.58 2231.39 1545.58 Q2235.4 1545.58 2237.74 1543.48 Q2240.07 1541.37 2240.07 1537.76 Q2240.07 1534.15 2237.74 1532.04 Q2235.4 1529.94 2231.39 1529.94 Q2229.52 1529.94 2227.64 1530.35 Q2225.79 1530.77 2223.85 1531.65 L2223.85 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M2263.96 1517.37 Q2260.35 1517.37 2258.52 1520.93 Q2256.72 1524.47 2256.72 1531.6 Q2256.72 1538.71 2258.52 1542.27 Q2260.35 1545.82 2263.96 1545.82 Q2267.6 1545.82 2269.4 1542.27 Q2271.23 1538.71 2271.23 1531.6 Q2271.23 1524.47 2269.4 1520.93 Q2267.6 1517.37 2263.96 1517.37 M2263.96 1513.66 Q2269.77 1513.66 2272.83 1518.27 Q2275.91 1522.85 2275.91 1531.6 Q2275.91 1540.33 2272.83 1544.94 Q2269.77 1549.52 2263.96 1549.52 Q2258.15 1549.52 2255.07 1544.94 Q2252.02 1540.33 2252.02 1531.6 Q2252.02 1522.85 2255.07 1518.27 Q2258.15 1513.66 2263.96 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.274,1282.78 2352.76,1282.78 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.274,1038.84 2352.76,1038.84 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.274,794.891 2352.76,794.891 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.274,550.945 2352.76,550.945 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.274,306.998 2352.76,306.998 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip302)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.274,63.0515 2352.76,63.0515 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,1486.45 156.274,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,1282.78 175.172,1282.78 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,1038.84 175.172,1038.84 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,794.891 175.172,794.891 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,550.945 175.172,550.945 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,306.998 175.172,306.998 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.274,63.0515 175.172,63.0515 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip300)\" d=\"M62.9365 1268.58 Q59.3254 1268.58 57.4967 1272.15 Q55.6912 1275.69 55.6912 1282.82 Q55.6912 1289.93 57.4967 1293.49 Q59.3254 1297.03 62.9365 1297.03 Q66.5707 1297.03 68.3763 1293.49 Q70.205 1289.93 70.205 1282.82 Q70.205 1275.69 68.3763 1272.15 Q66.5707 1268.58 62.9365 1268.58 M62.9365 1264.88 Q68.7467 1264.88 71.8022 1269.49 Q74.8809 1274.07 74.8809 1282.82 Q74.8809 1291.55 71.8022 1296.15 Q68.7467 1300.74 62.9365 1300.74 Q57.1264 1300.74 54.0477 1296.15 Q50.9921 1291.55 50.9921 1282.82 Q50.9921 1274.07 54.0477 1269.49 Q57.1264 1264.88 62.9365 1264.88 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M83.0984 1294.18 L87.9827 1294.18 L87.9827 1300.06 L83.0984 1300.06 L83.0984 1294.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M108.746 1280.92 Q105.598 1280.92 103.746 1283.07 Q101.918 1285.23 101.918 1288.98 Q101.918 1292.7 103.746 1294.88 Q105.598 1297.03 108.746 1297.03 Q111.895 1297.03 113.723 1294.88 Q115.575 1292.7 115.575 1288.98 Q115.575 1285.23 113.723 1283.07 Q111.895 1280.92 108.746 1280.92 M118.029 1266.27 L118.029 1270.53 Q116.27 1269.69 114.464 1269.25 Q112.682 1268.81 110.922 1268.81 Q106.293 1268.81 103.839 1271.94 Q101.409 1275.06 101.061 1281.38 Q102.427 1279.37 104.487 1278.3 Q106.547 1277.22 109.024 1277.22 Q114.233 1277.22 117.242 1280.39 Q120.274 1283.54 120.274 1288.98 Q120.274 1294.3 117.126 1297.52 Q113.978 1300.74 108.746 1300.74 Q102.751 1300.74 99.5798 1296.15 Q96.4085 1291.55 96.4085 1282.82 Q96.4085 1274.62 100.297 1269.76 Q104.186 1264.88 110.737 1264.88 Q112.496 1264.88 114.279 1265.23 Q116.084 1265.57 118.029 1266.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M64.0013 1024.64 Q60.3902 1024.64 58.5615 1028.2 Q56.756 1031.74 56.756 1038.87 Q56.756 1045.98 58.5615 1049.54 Q60.3902 1053.09 64.0013 1053.09 Q67.6356 1053.09 69.4411 1049.54 Q71.2698 1045.98 71.2698 1038.87 Q71.2698 1031.74 69.4411 1028.2 Q67.6356 1024.64 64.0013 1024.64 M64.0013 1020.93 Q69.8115 1020.93 72.867 1025.54 Q75.9457 1030.12 75.9457 1038.87 Q75.9457 1047.6 72.867 1052.21 Q69.8115 1056.79 64.0013 1056.79 Q58.1912 1056.79 55.1125 1052.21 Q52.0569 1047.6 52.0569 1038.87 Q52.0569 1030.12 55.1125 1025.54 Q58.1912 1020.93 64.0013 1020.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M84.1632 1050.24 L89.0475 1050.24 L89.0475 1056.12 L84.1632 1056.12 L84.1632 1050.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M98.0521 1021.56 L120.274 1021.56 L120.274 1023.55 L107.728 1056.12 L102.844 1056.12 L114.649 1025.49 L98.0521 1025.49 L98.0521 1021.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M63.1911 780.69 Q59.58 780.69 57.7513 784.255 Q55.9458 787.796 55.9458 794.926 Q55.9458 802.032 57.7513 805.597 Q59.58 809.139 63.1911 809.139 Q66.8254 809.139 68.6309 805.597 Q70.4596 802.032 70.4596 794.926 Q70.4596 787.796 68.6309 784.255 Q66.8254 780.69 63.1911 780.69 M63.1911 776.986 Q69.0013 776.986 72.0568 781.592 Q75.1355 786.176 75.1355 794.926 Q75.1355 803.653 72.0568 808.259 Q69.0013 812.842 63.1911 812.842 Q57.381 812.842 54.3023 808.259 Q51.2468 803.653 51.2468 794.926 Q51.2468 786.176 54.3023 781.592 Q57.381 776.986 63.1911 776.986 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M83.3531 806.291 L88.2373 806.291 L88.2373 812.171 L83.3531 812.171 L83.3531 806.291 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M108.422 795.759 Q105.089 795.759 103.168 797.541 Q101.27 799.324 101.27 802.449 Q101.27 805.574 103.168 807.356 Q105.089 809.139 108.422 809.139 Q111.756 809.139 113.677 807.356 Q115.598 805.551 115.598 802.449 Q115.598 799.324 113.677 797.541 Q111.779 795.759 108.422 795.759 M103.746 793.768 Q100.737 793.028 99.0474 790.967 Q97.3808 788.907 97.3808 785.944 Q97.3808 781.801 100.321 779.393 Q103.284 776.986 108.422 776.986 Q113.584 776.986 116.524 779.393 Q119.464 781.801 119.464 785.944 Q119.464 788.907 117.774 790.967 Q116.108 793.028 113.121 793.768 Q116.501 794.555 118.376 796.847 Q120.274 799.139 120.274 802.449 Q120.274 807.472 117.195 810.157 Q114.14 812.842 108.422 812.842 Q102.705 812.842 99.6261 810.157 Q96.5706 807.472 96.5706 802.449 Q96.5706 799.139 98.4687 796.847 Q100.367 794.555 103.746 793.768 M102.034 786.384 Q102.034 789.069 103.7 790.574 Q105.39 792.079 108.422 792.079 Q111.432 792.079 113.121 790.574 Q114.834 789.069 114.834 786.384 Q114.834 783.699 113.121 782.194 Q111.432 780.69 108.422 780.69 Q105.39 780.69 103.7 782.194 Q102.034 783.699 102.034 786.384 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M63.2837 536.743 Q59.6726 536.743 57.8439 540.308 Q56.0384 543.85 56.0384 550.979 Q56.0384 558.086 57.8439 561.65 Q59.6726 565.192 63.2837 565.192 Q66.918 565.192 68.7235 561.65 Q70.5522 558.086 70.5522 550.979 Q70.5522 543.85 68.7235 540.308 Q66.918 536.743 63.2837 536.743 M63.2837 533.04 Q69.0939 533.04 72.1494 537.646 Q75.2281 542.229 75.2281 550.979 Q75.2281 559.706 72.1494 564.313 Q69.0939 568.896 63.2837 568.896 Q57.4736 568.896 54.3949 564.313 Q51.3393 559.706 51.3393 550.979 Q51.3393 542.229 54.3949 537.646 Q57.4736 533.04 63.2837 533.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M83.4457 562.345 L88.3299 562.345 L88.3299 568.225 L83.4457 568.225 L83.4457 562.345 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M98.6539 567.507 L98.6539 563.248 Q100.413 564.081 102.219 564.521 Q104.024 564.961 105.76 564.961 Q110.39 564.961 112.82 561.859 Q115.274 558.734 115.621 552.391 Q114.279 554.382 112.219 555.447 Q110.158 556.512 107.658 556.512 Q102.473 556.512 99.4409 553.387 Q96.4317 550.239 96.4317 544.799 Q96.4317 539.475 99.5798 536.257 Q102.728 533.04 107.959 533.04 Q113.955 533.04 117.103 537.646 Q120.274 542.229 120.274 550.979 Q120.274 559.15 116.385 564.035 Q112.52 568.896 105.969 568.896 Q104.209 568.896 102.404 568.549 Q100.598 568.201 98.6539 567.507 M107.959 552.854 Q111.108 552.854 112.936 550.701 Q114.788 548.549 114.788 544.799 Q114.788 541.072 112.936 538.919 Q111.108 536.743 107.959 536.743 Q104.811 536.743 102.959 538.919 Q101.131 541.072 101.131 544.799 Q101.131 548.549 102.959 550.701 Q104.811 552.854 107.959 552.854 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M53.9088 320.343 L61.5476 320.343 L61.5476 293.977 L53.2375 295.644 L53.2375 291.385 L61.5013 289.718 L66.1772 289.718 L66.1772 320.343 L73.8161 320.343 L73.8161 324.278 L53.9088 324.278 L53.9088 320.343 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M83.2605 318.398 L88.1447 318.398 L88.1447 324.278 L83.2605 324.278 L83.2605 318.398 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M108.33 292.797 Q104.719 292.797 102.89 296.361 Q101.084 299.903 101.084 307.033 Q101.084 314.139 102.89 317.704 Q104.719 321.246 108.33 321.246 Q111.964 321.246 113.77 317.704 Q115.598 314.139 115.598 307.033 Q115.598 299.903 113.77 296.361 Q111.964 292.797 108.33 292.797 M108.33 289.093 Q114.14 289.093 117.195 293.699 Q120.274 298.283 120.274 307.033 Q120.274 315.76 117.195 320.366 Q114.14 324.949 108.33 324.949 Q102.52 324.949 99.4409 320.366 Q96.3854 315.76 96.3854 307.033 Q96.3854 298.283 99.4409 293.699 Q102.52 289.093 108.33 289.093 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M55.1356 76.3963 L62.7745 76.3963 L62.7745 50.0307 L54.4643 51.6974 L54.4643 47.4382 L62.7282 45.7715 L67.4041 45.7715 L67.4041 76.3963 L75.0429 76.3963 L75.0429 80.3315 L55.1356 80.3315 L55.1356 76.3963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M84.4873 74.4519 L89.3715 74.4519 L89.3715 80.3315 L84.4873 80.3315 L84.4873 74.4519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M100.367 76.3963 L108.006 76.3963 L108.006 50.0307 L99.6956 51.6974 L99.6956 47.4382 L107.959 45.7715 L112.635 45.7715 L112.635 76.3963 L120.274 76.3963 L120.274 80.3315 L100.367 80.3315 L100.367 76.3963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip302)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  218.439,100.322 259.882,87.9763 301.325,157.371 342.768,236.903 384.211,328.482 425.654,348.623 467.097,499.876 508.54,453.347 549.983,778.829 591.426,644.469 \n",
       "  632.869,791.522 674.312,855.924 715.755,654.454 757.198,706.59 798.641,1028.21 840.085,885.771 881.528,969.443 922.971,677.886 964.414,714.369 1005.86,1262.72 \n",
       "  1047.3,1350.31 1088.74,1067.93 1130.19,1142.26 1171.63,1445.72 1213.07,906.967 1254.52,987.567 1295.96,994.563 1337.4,818.355 1378.84,909.232 1420.29,838.239 \n",
       "  1461.73,1042.65 1503.17,1181.2 1544.62,769.991 1586.06,676.688 1627.5,1060.05 1668.95,853.836 1710.39,1121.5 1751.83,1085.17 1793.27,1119.96 1834.72,1199.31 \n",
       "  1876.16,1054.5 1917.6,1135.86 1959.05,974.472 2000.49,1349.12 2041.93,939.721 2083.38,1290.22 2124.82,1077.91 2166.26,1062.17 2207.71,1121.25 2249.15,1089.21 \n",
       "  2290.59,1305.91 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip300)\" d=\"\n",
       "M1983.06 198.898 L2279.54 198.898 L2279.54 95.2176 L1983.06 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1983.06,198.898 2279.54,198.898 2279.54,95.2176 1983.06,95.2176 1983.06,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip300)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.46,147.058 2153.89,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip300)\" d=\"M2192.14 166.745 Q2190.34 171.375 2188.62 172.787 Q2186.91 174.199 2184.04 174.199 L2180.64 174.199 L2180.64 170.634 L2183.14 170.634 Q2184.9 170.634 2185.87 169.8 Q2186.84 168.967 2188.02 165.865 L2188.78 163.921 L2178.3 138.412 L2182.81 138.412 L2190.91 158.689 L2199.02 138.412 L2203.53 138.412 L2192.14 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip300)\" d=\"M2210.82 160.402 L2218.46 160.402 L2218.46 134.037 L2210.15 135.703 L2210.15 131.444 L2218.41 129.778 L2223.09 129.778 L2223.09 160.402 L2230.73 160.402 L2230.73 164.338 L2210.82 164.338 L2210.82 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(report(mach).training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericRange(1 ≤ epochs ≤ 50; origin=25.5, unit=24.5)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(model, :epochs, lower=1, upper=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{ProbabilisticTunedModel{Grid,…},…}.\n",
      "└ @ MLJBase /home/john/.julia/packages/MLJBase/QXObv/src/machines.jl:403\n",
      "┌ Info: Attempting to evaluate 30 models.\n",
      "└ @ MLJTuning /home/john/.julia/packages/MLJTuning/bjRHJ/src/tuned_models.jl:680\n",
      "\u001b[33mEvaluating over 30 metamodels: 100%[=========================] Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"epochs\",\n",
       " parameter_scale = :linear,\n",
       " parameter_values = [1, 3, 4, 6, 8, 9, 11, 13, 15, 16  …  35, 36, 38, 40, 42, 43, 45, 47, 48, 50],\n",
       " measurements = [0.9642334968383826, 0.7611330568962873, 0.7020568931803699, 0.632943642719447, 0.6169772135379632, 0.5928938750629911, 0.5644106692422063, 0.5619940401803774, 0.571280115615697, 0.5638080939491867  …  0.44223591588036054, 0.43066961955183003, 0.4555875545087298, 0.41438433156376586, 0.3912311905665708, 0.4119545819959012, 0.428914801843594, 0.4264896038319531, 0.4099500504910243, 0.4021205125391061],)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip340\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip341\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M219.866 1423.18 L2352.76 1423.18 L2352.76 47.2441 L219.866 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip342\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  239.167,1423.18 239.167,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  649.811,1423.18 649.811,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1060.46,1423.18 1060.46,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1471.1,1423.18 1471.1,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1881.75,1423.18 1881.75,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.39,1423.18 2292.39,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  239.167,1423.18 239.167,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  649.811,1423.18 649.811,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1060.46,1423.18 1060.46,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1471.1,1423.18 1471.1,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1881.75,1423.18 1881.75,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.39,1423.18 2292.39,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M239.167 1454.1 Q235.555 1454.1 233.727 1457.66 Q231.921 1461.2 231.921 1468.33 Q231.921 1475.44 233.727 1479.01 Q235.555 1482.55 239.167 1482.55 Q242.801 1482.55 244.606 1479.01 Q246.435 1475.44 246.435 1468.33 Q246.435 1461.2 244.606 1457.66 Q242.801 1454.1 239.167 1454.1 M239.167 1450.39 Q244.977 1450.39 248.032 1455 Q251.111 1459.58 251.111 1468.33 Q251.111 1477.06 248.032 1481.67 Q244.977 1486.25 239.167 1486.25 Q233.356 1486.25 230.278 1481.67 Q227.222 1477.06 227.222 1468.33 Q227.222 1459.58 230.278 1455 Q233.356 1450.39 239.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M624.499 1481.64 L632.138 1481.64 L632.138 1455.28 L623.828 1456.95 L623.828 1452.69 L632.092 1451.02 L636.768 1451.02 L636.768 1481.64 L644.406 1481.64 L644.406 1485.58 L624.499 1485.58 L624.499 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M663.851 1454.1 Q660.24 1454.1 658.411 1457.66 Q656.605 1461.2 656.605 1468.33 Q656.605 1475.44 658.411 1479.01 Q660.24 1482.55 663.851 1482.55 Q667.485 1482.55 669.291 1479.01 Q671.119 1475.44 671.119 1468.33 Q671.119 1461.2 669.291 1457.66 Q667.485 1454.1 663.851 1454.1 M663.851 1450.39 Q669.661 1450.39 672.716 1455 Q675.795 1459.58 675.795 1468.33 Q675.795 1477.06 672.716 1481.67 Q669.661 1486.25 663.851 1486.25 Q658.041 1486.25 654.962 1481.67 Q651.906 1477.06 651.906 1468.33 Q651.906 1459.58 654.962 1455 Q658.041 1450.39 663.851 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1039.23 1481.64 L1055.55 1481.64 L1055.55 1485.58 L1033.6 1485.58 L1033.6 1481.64 Q1036.27 1478.89 1040.85 1474.26 Q1045.46 1469.61 1046.64 1468.27 Q1048.88 1465.74 1049.76 1464.01 Q1050.66 1462.25 1050.66 1460.56 Q1050.66 1457.8 1048.72 1456.07 Q1046.8 1454.33 1043.7 1454.33 Q1041.5 1454.33 1039.04 1455.09 Q1036.61 1455.86 1033.84 1457.41 L1033.84 1452.69 Q1036.66 1451.55 1039.11 1450.97 Q1041.57 1450.39 1043.6 1450.39 Q1048.97 1450.39 1052.17 1453.08 Q1055.36 1455.77 1055.36 1460.26 Q1055.36 1462.39 1054.55 1464.31 Q1053.77 1466.2 1051.66 1468.8 Q1051.08 1469.47 1047.98 1472.69 Q1044.88 1475.88 1039.23 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1075.36 1454.1 Q1071.75 1454.1 1069.92 1457.66 Q1068.12 1461.2 1068.12 1468.33 Q1068.12 1475.44 1069.92 1479.01 Q1071.75 1482.55 1075.36 1482.55 Q1079 1482.55 1080.8 1479.01 Q1082.63 1475.44 1082.63 1468.33 Q1082.63 1461.2 1080.8 1457.66 Q1079 1454.1 1075.36 1454.1 M1075.36 1450.39 Q1081.17 1450.39 1084.23 1455 Q1087.31 1459.58 1087.31 1468.33 Q1087.31 1477.06 1084.23 1481.67 Q1081.17 1486.25 1075.36 1486.25 Q1069.55 1486.25 1066.47 1481.67 Q1063.42 1477.06 1063.42 1468.33 Q1063.42 1459.58 1066.47 1455 Q1069.55 1450.39 1075.36 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1459.94 1466.95 Q1463.3 1467.66 1465.18 1469.93 Q1467.07 1472.2 1467.07 1475.53 Q1467.07 1480.65 1463.56 1483.45 Q1460.04 1486.25 1453.56 1486.25 Q1451.38 1486.25 1449.06 1485.81 Q1446.77 1485.39 1444.32 1484.54 L1444.32 1480.02 Q1446.26 1481.16 1448.58 1481.74 Q1450.89 1482.32 1453.42 1482.32 Q1457.81 1482.32 1460.11 1480.58 Q1462.42 1478.84 1462.42 1475.53 Q1462.42 1472.48 1460.27 1470.77 Q1458.14 1469.03 1454.32 1469.03 L1450.29 1469.03 L1450.29 1465.19 L1454.5 1465.19 Q1457.95 1465.19 1459.78 1463.82 Q1461.61 1462.43 1461.61 1459.84 Q1461.61 1457.18 1459.71 1455.77 Q1457.84 1454.33 1454.32 1454.33 Q1452.4 1454.33 1450.2 1454.75 Q1448 1455.16 1445.36 1456.04 L1445.36 1451.88 Q1448.02 1451.14 1450.34 1450.77 Q1452.68 1450.39 1454.74 1450.39 Q1460.06 1450.39 1463.16 1452.83 Q1466.26 1455.23 1466.26 1459.35 Q1466.26 1462.22 1464.62 1464.21 Q1462.98 1466.18 1459.94 1466.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1485.94 1454.1 Q1482.33 1454.1 1480.5 1457.66 Q1478.69 1461.2 1478.69 1468.33 Q1478.69 1475.44 1480.5 1479.01 Q1482.33 1482.55 1485.94 1482.55 Q1489.57 1482.55 1491.38 1479.01 Q1493.21 1475.44 1493.21 1468.33 Q1493.21 1461.2 1491.38 1457.66 Q1489.57 1454.1 1485.94 1454.1 M1485.94 1450.39 Q1491.75 1450.39 1494.8 1455 Q1497.88 1459.58 1497.88 1468.33 Q1497.88 1477.06 1494.8 1481.67 Q1491.75 1486.25 1485.94 1486.25 Q1480.13 1486.25 1477.05 1481.67 Q1473.99 1477.06 1473.99 1468.33 Q1473.99 1459.58 1477.05 1455 Q1480.13 1450.39 1485.94 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1869.92 1455.09 L1858.11 1473.54 L1869.92 1473.54 L1869.92 1455.09 M1868.69 1451.02 L1874.57 1451.02 L1874.57 1473.54 L1879.5 1473.54 L1879.5 1477.43 L1874.57 1477.43 L1874.57 1485.58 L1869.92 1485.58 L1869.92 1477.43 L1854.32 1477.43 L1854.32 1472.92 L1868.69 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1897.23 1454.1 Q1893.62 1454.1 1891.79 1457.66 Q1889.99 1461.2 1889.99 1468.33 Q1889.99 1475.44 1891.79 1479.01 Q1893.62 1482.55 1897.23 1482.55 Q1900.87 1482.55 1902.67 1479.01 Q1904.5 1475.44 1904.5 1468.33 Q1904.5 1461.2 1902.67 1457.66 Q1900.87 1454.1 1897.23 1454.1 M1897.23 1450.39 Q1903.04 1450.39 1906.1 1455 Q1909.18 1459.58 1909.18 1468.33 Q1909.18 1477.06 1906.1 1481.67 Q1903.04 1486.25 1897.23 1486.25 Q1891.42 1486.25 1888.34 1481.67 Q1885.29 1477.06 1885.29 1468.33 Q1885.29 1459.58 1888.34 1455 Q1891.42 1450.39 1897.23 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2267.09 1451.02 L2285.45 1451.02 L2285.45 1454.96 L2271.37 1454.96 L2271.37 1463.43 Q2272.39 1463.08 2273.41 1462.92 Q2274.43 1462.73 2275.45 1462.73 Q2281.23 1462.73 2284.61 1465.9 Q2287.99 1469.08 2287.99 1474.49 Q2287.99 1480.07 2284.52 1483.17 Q2281.05 1486.25 2274.73 1486.25 Q2272.55 1486.25 2270.28 1485.88 Q2268.04 1485.51 2265.63 1484.77 L2265.63 1480.07 Q2267.72 1481.2 2269.94 1481.76 Q2272.16 1482.32 2274.64 1482.32 Q2278.64 1482.32 2280.98 1480.21 Q2283.32 1478.1 2283.32 1474.49 Q2283.32 1470.88 2280.98 1468.77 Q2278.64 1466.67 2274.64 1466.67 Q2272.76 1466.67 2270.89 1467.08 Q2269.03 1467.5 2267.09 1468.38 L2267.09 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2307.21 1454.1 Q2303.59 1454.1 2301.77 1457.66 Q2299.96 1461.2 2299.96 1468.33 Q2299.96 1475.44 2301.77 1479.01 Q2303.59 1482.55 2307.21 1482.55 Q2310.84 1482.55 2312.65 1479.01 Q2314.47 1475.44 2314.47 1468.33 Q2314.47 1461.2 2312.65 1457.66 Q2310.84 1454.1 2307.21 1454.1 M2307.21 1450.39 Q2313.02 1450.39 2316.07 1455 Q2319.15 1459.58 2319.15 1468.33 Q2319.15 1477.06 2316.07 1481.67 Q2313.02 1486.25 2307.21 1486.25 Q2301.4 1486.25 2298.32 1481.67 Q2295.26 1477.06 2295.26 1468.33 Q2295.26 1459.58 2298.32 1455 Q2301.4 1450.39 2307.21 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1206.5 1548.76 L1206.5 1551.62 L1179.57 1551.62 Q1179.96 1557.67 1183.2 1560.85 Q1186.48 1564 1192.31 1564 Q1195.68 1564 1198.83 1563.17 Q1202.01 1562.35 1205.13 1560.69 L1205.13 1566.23 Q1201.98 1567.57 1198.67 1568.27 Q1195.36 1568.97 1191.96 1568.97 Q1183.43 1568.97 1178.43 1564 Q1173.46 1559.04 1173.46 1550.57 Q1173.46 1541.82 1178.17 1536.69 Q1182.92 1531.54 1190.94 1531.54 Q1198.13 1531.54 1202.3 1536.18 Q1206.5 1540.8 1206.5 1548.76 M1200.64 1547.04 Q1200.58 1542.23 1197.94 1539.37 Q1195.33 1536.5 1191 1536.5 Q1186.1 1536.5 1183.14 1539.27 Q1180.21 1542.04 1179.77 1547.07 L1200.64 1547.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1221.78 1562.7 L1221.78 1581.6 L1215.89 1581.6 L1215.89 1532.4 L1221.78 1532.4 L1221.78 1537.81 Q1223.62 1534.62 1226.43 1533.1 Q1229.26 1531.54 1233.17 1531.54 Q1239.67 1531.54 1243.71 1536.69 Q1247.78 1541.85 1247.78 1550.25 Q1247.78 1558.65 1243.71 1563.81 Q1239.67 1568.97 1233.17 1568.97 Q1229.26 1568.97 1226.43 1567.44 Q1223.62 1565.88 1221.78 1562.7 M1241.7 1550.25 Q1241.7 1543.79 1239.03 1540.13 Q1236.39 1536.44 1231.74 1536.44 Q1227.09 1536.44 1224.42 1540.13 Q1221.78 1543.79 1221.78 1550.25 Q1221.78 1556.71 1224.42 1560.4 Q1227.09 1564.07 1231.74 1564.07 Q1236.39 1564.07 1239.03 1560.4 Q1241.7 1556.71 1241.7 1550.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1271.3 1536.5 Q1266.59 1536.5 1263.86 1540.19 Q1261.12 1543.85 1261.12 1550.25 Q1261.12 1556.65 1263.82 1560.34 Q1266.56 1564 1271.3 1564 Q1275.98 1564 1278.72 1560.31 Q1281.46 1556.62 1281.46 1550.25 Q1281.46 1543.92 1278.72 1540.23 Q1275.98 1536.5 1271.3 1536.5 M1271.3 1531.54 Q1278.94 1531.54 1283.3 1536.5 Q1287.66 1541.47 1287.66 1550.25 Q1287.66 1559 1283.3 1564 Q1278.94 1568.97 1271.3 1568.97 Q1263.63 1568.97 1259.27 1564 Q1254.94 1559 1254.94 1550.25 Q1254.94 1541.47 1259.27 1536.5 Q1263.63 1531.54 1271.3 1531.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1323.03 1533.76 L1323.03 1539.24 Q1320.54 1537.87 1318.03 1537.2 Q1315.55 1536.5 1313 1536.5 Q1307.3 1536.5 1304.15 1540.13 Q1301 1543.73 1301 1550.25 Q1301 1556.78 1304.15 1560.4 Q1307.3 1564 1313 1564 Q1315.55 1564 1318.03 1563.33 Q1320.54 1562.63 1323.03 1561.26 L1323.03 1566.68 Q1320.57 1567.82 1317.93 1568.39 Q1315.32 1568.97 1312.36 1568.97 Q1304.31 1568.97 1299.57 1563.91 Q1294.83 1558.85 1294.83 1550.25 Q1294.83 1541.53 1299.6 1536.53 Q1304.41 1531.54 1312.74 1531.54 Q1315.45 1531.54 1318.03 1532.11 Q1320.61 1532.65 1323.03 1533.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1362.84 1546.53 L1362.84 1568.04 L1356.99 1568.04 L1356.99 1546.72 Q1356.99 1541.66 1355.01 1539.14 Q1353.04 1536.63 1349.09 1536.63 Q1344.35 1536.63 1341.61 1539.65 Q1338.88 1542.68 1338.88 1547.9 L1338.88 1568.04 L1332.99 1568.04 L1332.99 1518.52 L1338.88 1518.52 L1338.88 1537.93 Q1340.98 1534.72 1343.81 1533.13 Q1346.67 1531.54 1350.4 1531.54 Q1356.54 1531.54 1359.69 1535.36 Q1362.84 1539.14 1362.84 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1397.25 1533.45 L1397.25 1538.98 Q1394.77 1537.71 1392.09 1537.07 Q1389.42 1536.44 1386.56 1536.44 Q1382.19 1536.44 1380 1537.77 Q1377.83 1539.11 1377.83 1541.79 Q1377.83 1543.82 1379.39 1545 Q1380.95 1546.15 1385.66 1547.2 L1387.67 1547.64 Q1393.91 1548.98 1396.52 1551.43 Q1399.16 1553.85 1399.16 1558.21 Q1399.16 1563.17 1395.21 1566.07 Q1391.3 1568.97 1384.42 1568.97 Q1381.56 1568.97 1378.44 1568.39 Q1375.35 1567.85 1371.91 1566.74 L1371.91 1560.69 Q1375.16 1562.38 1378.31 1563.24 Q1381.46 1564.07 1384.55 1564.07 Q1388.69 1564.07 1390.92 1562.66 Q1393.14 1561.23 1393.14 1558.65 Q1393.14 1556.27 1391.52 1554.99 Q1389.93 1553.72 1384.49 1552.54 L1382.45 1552.07 Q1377.01 1550.92 1374.59 1548.56 Q1372.17 1546.18 1372.17 1542.04 Q1372.17 1537.01 1375.73 1534.27 Q1379.3 1531.54 1385.85 1531.54 Q1389.1 1531.54 1391.97 1532.01 Q1394.83 1532.49 1397.25 1533.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,1315.43 2352.76,1315.43 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,1088.79 2352.76,1088.79 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,862.151 2352.76,862.151 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,635.509 2352.76,635.509 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,408.867 2352.76,408.867 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,182.225 2352.76,182.225 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1423.18 219.866,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1315.43 238.764,1315.43 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1088.79 238.764,1088.79 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,862.151 238.764,862.151 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,635.509 238.764,635.509 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,408.867 238.764,408.867 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,182.225 238.764,182.225 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M126.205 1301.23 Q122.593 1301.23 120.765 1304.8 Q118.959 1308.34 118.959 1315.47 Q118.959 1322.58 120.765 1326.14 Q122.593 1329.68 126.205 1329.68 Q129.839 1329.68 131.644 1326.14 Q133.473 1322.58 133.473 1315.47 Q133.473 1308.34 131.644 1304.8 Q129.839 1301.23 126.205 1301.23 M126.205 1297.53 Q132.015 1297.53 135.07 1302.14 Q138.149 1306.72 138.149 1315.47 Q138.149 1324.2 135.07 1328.8 Q132.015 1333.39 126.205 1333.39 Q120.394 1333.39 117.316 1328.8 Q114.26 1324.2 114.26 1315.47 Q114.26 1306.72 117.316 1302.14 Q120.394 1297.53 126.205 1297.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M146.366 1326.84 L151.251 1326.84 L151.251 1332.71 L146.366 1332.71 L146.366 1326.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M174.283 1302.23 L162.477 1320.68 L174.283 1320.68 L174.283 1302.23 M173.056 1298.15 L178.936 1298.15 L178.936 1320.68 L183.866 1320.68 L183.866 1324.57 L178.936 1324.57 L178.936 1332.71 L174.283 1332.71 L174.283 1324.57 L158.681 1324.57 L158.681 1320.05 L173.056 1298.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M127.686 1074.59 Q124.075 1074.59 122.246 1078.16 Q120.441 1081.7 120.441 1088.83 Q120.441 1095.93 122.246 1099.5 Q124.075 1103.04 127.686 1103.04 Q131.32 1103.04 133.126 1099.5 Q134.954 1095.93 134.954 1088.83 Q134.954 1081.7 133.126 1078.16 Q131.32 1074.59 127.686 1074.59 M127.686 1070.89 Q133.496 1070.89 136.552 1075.49 Q139.63 1080.08 139.63 1088.83 Q139.63 1097.55 136.552 1102.16 Q133.496 1106.74 127.686 1106.74 Q121.876 1106.74 118.797 1102.16 Q115.742 1097.55 115.742 1088.83 Q115.742 1080.08 118.797 1075.49 Q121.876 1070.89 127.686 1070.89 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M147.848 1100.19 L152.732 1100.19 L152.732 1106.07 L147.848 1106.07 L147.848 1100.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M162.964 1071.51 L181.32 1071.51 L181.32 1075.45 L167.246 1075.45 L167.246 1083.92 Q168.264 1083.57 169.283 1083.41 Q170.302 1083.23 171.32 1083.23 Q177.107 1083.23 180.487 1086.4 Q183.866 1089.57 183.866 1094.98 Q183.866 1100.56 180.394 1103.67 Q176.922 1106.74 170.602 1106.74 Q168.427 1106.74 166.158 1106.37 Q163.913 1106 161.505 1105.26 L161.505 1100.56 Q163.589 1101.7 165.811 1102.25 Q168.033 1102.81 170.51 1102.81 Q174.514 1102.81 176.852 1100.7 Q179.19 1098.6 179.19 1094.98 Q179.19 1091.37 176.852 1089.27 Q174.514 1087.16 170.51 1087.16 Q168.635 1087.16 166.76 1087.58 Q164.908 1087.99 162.964 1088.87 L162.964 1071.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M126.529 847.949 Q122.918 847.949 121.089 851.514 Q119.283 855.056 119.283 862.185 Q119.283 869.292 121.089 872.857 Q122.918 876.398 126.529 876.398 Q130.163 876.398 131.968 872.857 Q133.797 869.292 133.797 862.185 Q133.797 855.056 131.968 851.514 Q130.163 847.949 126.529 847.949 M126.529 844.246 Q132.339 844.246 135.394 848.852 Q138.473 853.436 138.473 862.185 Q138.473 870.912 135.394 875.519 Q132.339 880.102 126.529 880.102 Q120.718 880.102 117.64 875.519 Q114.584 870.912 114.584 862.185 Q114.584 853.436 117.64 848.852 Q120.718 844.246 126.529 844.246 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M146.691 873.551 L151.575 873.551 L151.575 879.431 L146.691 879.431 L146.691 873.551 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M172.339 860.287 Q169.19 860.287 167.339 862.44 Q165.51 864.593 165.51 868.343 Q165.51 872.07 167.339 874.246 Q169.19 876.398 172.339 876.398 Q175.487 876.398 177.315 874.246 Q179.167 872.07 179.167 868.343 Q179.167 864.593 177.315 862.44 Q175.487 860.287 172.339 860.287 M181.621 845.635 L181.621 849.894 Q179.862 849.061 178.056 848.621 Q176.274 848.181 174.514 848.181 Q169.885 848.181 167.431 851.306 Q165.001 854.431 164.653 860.75 Q166.019 858.736 168.079 857.672 Q170.139 856.584 172.616 856.584 Q177.825 856.584 180.834 859.755 Q183.866 862.903 183.866 868.343 Q183.866 873.667 180.718 876.884 Q177.57 880.102 172.339 880.102 Q166.343 880.102 163.172 875.519 Q160.001 870.912 160.001 862.185 Q160.001 853.991 163.89 849.13 Q167.778 844.246 174.329 844.246 Q176.089 844.246 177.871 844.593 Q179.676 844.94 181.621 845.635 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M127.593 621.307 Q123.982 621.307 122.154 624.872 Q120.348 628.414 120.348 635.543 Q120.348 642.65 122.154 646.215 Q123.982 649.756 127.593 649.756 Q131.228 649.756 133.033 646.215 Q134.862 642.65 134.862 635.543 Q134.862 628.414 133.033 624.872 Q131.228 621.307 127.593 621.307 M127.593 617.604 Q133.404 617.604 136.459 622.21 Q139.538 626.793 139.538 635.543 Q139.538 644.27 136.459 648.877 Q133.404 653.46 127.593 653.46 Q121.783 653.46 118.705 648.877 Q115.649 644.27 115.649 635.543 Q115.649 626.793 118.705 622.21 Q121.783 617.604 127.593 617.604 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M147.755 646.909 L152.64 646.909 L152.64 652.789 L147.755 652.789 L147.755 646.909 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M161.644 618.229 L183.866 618.229 L183.866 620.219 L171.32 652.789 L166.436 652.789 L178.241 622.164 L161.644 622.164 L161.644 618.229 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M126.783 394.665 Q123.172 394.665 121.343 398.23 Q119.538 401.772 119.538 408.901 Q119.538 416.008 121.343 419.573 Q123.172 423.114 126.783 423.114 Q130.417 423.114 132.223 419.573 Q134.052 416.008 134.052 408.901 Q134.052 401.772 132.223 398.23 Q130.417 394.665 126.783 394.665 M126.783 390.962 Q132.593 390.962 135.649 395.568 Q138.728 400.151 138.728 408.901 Q138.728 417.628 135.649 422.235 Q132.593 426.818 126.783 426.818 Q120.973 426.818 117.894 422.235 Q114.839 417.628 114.839 408.901 Q114.839 400.151 117.894 395.568 Q120.973 390.962 126.783 390.962 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M146.945 420.267 L151.829 420.267 L151.829 426.147 L146.945 426.147 L146.945 420.267 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M172.014 409.735 Q168.681 409.735 166.76 411.517 Q164.862 413.3 164.862 416.424 Q164.862 419.549 166.76 421.332 Q168.681 423.114 172.014 423.114 Q175.348 423.114 177.269 421.332 Q179.19 419.526 179.19 416.424 Q179.19 413.3 177.269 411.517 Q175.371 409.735 172.014 409.735 M167.339 407.744 Q164.329 407.003 162.64 404.943 Q160.973 402.883 160.973 399.92 Q160.973 395.776 163.913 393.369 Q166.876 390.962 172.014 390.962 Q177.176 390.962 180.116 393.369 Q183.056 395.776 183.056 399.92 Q183.056 402.883 181.366 404.943 Q179.7 407.003 176.714 407.744 Q180.093 408.531 181.968 410.823 Q183.866 413.114 183.866 416.424 Q183.866 421.448 180.788 424.133 Q177.732 426.818 172.014 426.818 Q166.297 426.818 163.218 424.133 Q160.163 421.448 160.163 416.424 Q160.163 413.114 162.061 410.823 Q163.959 408.531 167.339 407.744 M165.626 400.36 Q165.626 403.045 167.292 404.55 Q168.982 406.054 172.014 406.054 Q175.024 406.054 176.714 404.55 Q178.426 403.045 178.426 400.36 Q178.426 397.675 176.714 396.17 Q175.024 394.665 172.014 394.665 Q168.982 394.665 167.292 396.17 Q165.626 397.675 165.626 400.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M126.876 168.023 Q123.265 168.023 121.436 171.588 Q119.63 175.13 119.63 182.259 Q119.63 189.366 121.436 192.931 Q123.265 196.472 126.876 196.472 Q130.51 196.472 132.316 192.931 Q134.144 189.366 134.144 182.259 Q134.144 175.13 132.316 171.588 Q130.51 168.023 126.876 168.023 M126.876 164.32 Q132.686 164.32 135.742 168.926 Q138.82 173.509 138.82 182.259 Q138.82 190.986 135.742 195.593 Q132.686 200.176 126.876 200.176 Q121.066 200.176 117.987 195.593 Q114.931 190.986 114.931 182.259 Q114.931 173.509 117.987 168.926 Q121.066 164.32 126.876 164.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M147.038 193.625 L151.922 193.625 L151.922 199.505 L147.038 199.505 L147.038 193.625 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M162.246 198.787 L162.246 194.528 Q164.005 195.361 165.811 195.801 Q167.616 196.241 169.352 196.241 Q173.982 196.241 176.413 193.139 Q178.866 190.014 179.214 183.671 Q177.871 185.662 175.811 186.727 Q173.751 187.792 171.251 187.792 Q166.065 187.792 163.033 184.667 Q160.024 181.519 160.024 176.079 Q160.024 170.755 163.172 167.537 Q166.32 164.32 171.552 164.32 Q177.547 164.32 180.695 168.926 Q183.866 173.509 183.866 182.259 Q183.866 190.431 179.977 195.315 Q176.112 200.176 169.561 200.176 Q167.802 200.176 165.996 199.829 Q164.19 199.481 162.246 198.787 M171.552 184.134 Q174.7 184.134 176.528 181.982 Q178.38 179.829 178.38 176.079 Q178.38 172.352 176.528 170.199 Q174.7 168.023 171.552 168.023 Q168.403 168.023 166.552 170.199 Q164.723 172.352 164.723 176.079 Q164.723 179.829 166.552 181.982 Q168.403 184.134 171.552 184.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M29.7248 1170.59 L35.1993 1170.59 Q33.8307 1173.08 33.1623 1175.59 Q32.4621 1178.07 32.4621 1180.62 Q32.4621 1186.32 36.0905 1189.47 Q39.6872 1192.62 46.212 1192.62 Q52.7369 1192.62 56.3653 1189.47 Q59.9619 1186.32 59.9619 1180.62 Q59.9619 1178.07 59.2935 1175.59 Q58.5933 1173.08 57.2247 1170.59 L62.6355 1170.59 Q63.7814 1173.05 64.3543 1175.69 Q64.9272 1178.3 64.9272 1181.26 Q64.9272 1189.31 59.8664 1194.05 Q54.8057 1198.79 46.212 1198.79 Q37.491 1198.79 32.4939 1194.02 Q27.4968 1189.21 27.4968 1180.88 Q27.4968 1178.17 28.0697 1175.59 Q28.6108 1173.01 29.7248 1170.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M33.8307 1139.75 Q33.2578 1140.74 33.0032 1141.92 Q32.7167 1143.06 32.7167 1144.46 Q32.7167 1149.43 35.9632 1152.1 Q39.1779 1154.74 45.2253 1154.74 L64.0042 1154.74 L64.0042 1160.63 L28.3562 1160.63 L28.3562 1154.74 L33.8944 1154.74 Q30.6479 1152.9 29.0883 1149.94 Q27.4968 1146.98 27.4968 1142.74 Q27.4968 1142.14 27.5923 1141.41 Q27.656 1140.68 27.8151 1139.78 L33.8307 1139.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M32.4621 1121.23 Q32.4621 1125.94 36.1542 1128.68 Q39.8145 1131.41 46.212 1131.41 Q52.6095 1131.41 56.3017 1128.71 Q59.9619 1125.97 59.9619 1121.23 Q59.9619 1116.55 56.2698 1113.81 Q52.5777 1111.08 46.212 1111.08 Q39.8781 1111.08 36.186 1113.81 Q32.4621 1116.55 32.4621 1121.23 M27.4968 1121.23 Q27.4968 1113.59 32.4621 1109.23 Q37.4273 1104.87 46.212 1104.87 Q54.9649 1104.87 59.9619 1109.23 Q64.9272 1113.59 64.9272 1121.23 Q64.9272 1128.9 59.9619 1133.26 Q54.9649 1137.59 46.212 1137.59 Q37.4273 1137.59 32.4621 1133.26 Q27.4968 1128.9 27.4968 1121.23 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M29.4065 1072.44 L34.9447 1072.44 Q33.6716 1074.92 33.035 1077.59 Q32.3984 1080.27 32.3984 1083.13 Q32.3984 1087.49 33.7352 1089.69 Q35.072 1091.85 37.7456 1091.85 Q39.7826 1091.85 40.9603 1090.29 Q42.1061 1088.73 43.1565 1084.02 L43.6021 1082.02 Q44.9389 1075.78 47.3897 1073.17 Q49.8086 1070.53 54.1691 1070.53 Q59.1344 1070.53 62.0308 1074.47 Q64.9272 1078.39 64.9272 1085.26 Q64.9272 1088.13 64.3543 1091.25 Q63.8132 1094.33 62.6992 1097.77 L56.6518 1097.77 Q58.3387 1094.52 59.198 1091.37 Q60.0256 1088.22 60.0256 1085.13 Q60.0256 1081 58.6251 1078.77 Q57.1929 1076.54 54.6147 1076.54 Q52.2276 1076.54 50.9545 1078.16 Q49.6813 1079.76 48.5037 1085.2 L48.0262 1087.24 Q46.8804 1092.68 44.5251 1095.1 Q42.138 1097.52 38.0002 1097.52 Q32.9713 1097.52 30.2341 1093.95 Q27.4968 1090.39 27.4968 1083.83 Q27.4968 1080.58 27.9743 1077.72 Q28.4517 1074.85 29.4065 1072.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M29.4065 1038.47 L34.9447 1038.47 Q33.6716 1040.96 33.035 1043.63 Q32.3984 1046.3 32.3984 1049.17 Q32.3984 1053.53 33.7352 1055.73 Q35.072 1057.89 37.7456 1057.89 Q39.7826 1057.89 40.9603 1056.33 Q42.1061 1054.77 43.1565 1050.06 L43.6021 1048.05 Q44.9389 1041.82 47.3897 1039.21 Q49.8086 1036.56 54.1691 1036.56 Q59.1344 1036.56 62.0308 1040.51 Q64.9272 1044.43 64.9272 1051.3 Q64.9272 1054.17 64.3543 1057.28 Q63.8132 1060.37 62.6992 1063.81 L56.6518 1063.81 Q58.3387 1060.56 59.198 1057.41 Q60.0256 1054.26 60.0256 1051.17 Q60.0256 1047.04 58.6251 1044.81 Q57.1929 1042.58 54.6147 1042.58 Q52.2276 1042.58 50.9545 1044.2 Q49.6813 1045.79 48.5037 1051.24 L48.0262 1053.27 Q46.8804 1058.72 44.5251 1061.14 Q42.138 1063.56 38.0002 1063.56 Q32.9713 1063.56 30.2341 1059.99 Q27.4968 1056.43 27.4968 1049.87 Q27.4968 1046.62 27.9743 1043.76 Q28.4517 1040.89 29.4065 1038.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M44.7161 976.027 L47.5806 976.027 L47.5806 1002.95 Q53.6281 1002.57 56.8109 999.325 Q59.9619 996.047 59.9619 990.222 Q59.9619 986.848 59.1344 983.697 Q58.3069 980.514 56.6518 977.395 L62.1899 977.395 Q63.5267 980.546 64.227 983.856 Q64.9272 987.167 64.9272 990.572 Q64.9272 999.102 59.9619 1004.1 Q54.9967 1009.06 46.5303 1009.06 Q37.7774 1009.06 32.6531 1004.35 Q27.4968 999.612 27.4968 991.591 Q27.4968 984.398 32.1438 980.228 Q36.7589 976.027 44.7161 976.027 M42.9973 981.883 Q38.1912 981.947 35.3266 984.589 Q32.4621 987.198 32.4621 991.527 Q32.4621 996.429 35.2312 999.389 Q38.0002 1002.32 43.0292 1002.76 L42.9973 981.883 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M42.4881 936.782 L64.0042 936.782 L64.0042 942.639 L42.679 942.639 Q37.6183 942.639 35.1038 944.612 Q32.5894 946.585 32.5894 950.532 Q32.5894 955.274 35.6131 958.012 Q38.6368 960.749 43.8567 960.749 L64.0042 960.749 L64.0042 966.637 L28.3562 966.637 L28.3562 960.749 L33.8944 960.749 Q30.6797 958.648 29.0883 955.816 Q27.4968 952.951 27.4968 949.227 Q27.4968 943.084 31.3163 939.933 Q35.1038 936.782 42.4881 936.782 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M18.2347 919.308 L28.3562 919.308 L28.3562 907.245 L32.9077 907.245 L32.9077 919.308 L52.2594 919.308 Q56.6199 919.308 57.8613 918.131 Q59.1026 916.921 59.1026 913.261 L59.1026 907.245 L64.0042 907.245 L64.0042 913.261 Q64.0042 920.04 61.4897 922.618 Q58.9434 925.196 52.2594 925.196 L32.9077 925.196 L32.9077 929.493 L28.3562 929.493 L28.3562 925.196 L18.2347 925.196 L18.2347 919.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M33.8307 878.886 Q33.2578 879.873 33.0032 881.05 Q32.7167 882.196 32.7167 883.597 Q32.7167 888.562 35.9632 891.235 Q39.1779 893.877 45.2253 893.877 L64.0042 893.877 L64.0042 899.765 L28.3562 899.765 L28.3562 893.877 L33.8944 893.877 Q30.6479 892.031 29.0883 889.071 Q27.4968 886.111 27.4968 881.878 Q27.4968 881.273 27.5923 880.541 Q27.656 879.809 27.8151 878.918 L33.8307 878.886 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M32.4621 860.362 Q32.4621 865.072 36.1542 867.81 Q39.8145 870.547 46.212 870.547 Q52.6095 870.547 56.3017 867.841 Q59.9619 865.104 59.9619 860.362 Q59.9619 855.683 56.2698 852.946 Q52.5777 850.208 46.212 850.208 Q39.8781 850.208 36.186 852.946 Q32.4621 855.683 32.4621 860.362 M27.4968 860.362 Q27.4968 852.723 32.4621 848.362 Q37.4273 844.002 46.212 844.002 Q54.9649 844.002 59.9619 848.362 Q64.9272 852.723 64.9272 860.362 Q64.9272 868.032 59.9619 872.393 Q54.9649 876.722 46.212 876.722 Q37.4273 876.722 32.4621 872.393 Q27.4968 868.032 27.4968 860.362 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M58.657 828.629 L77.5631 828.629 L77.5631 834.517 L28.3562 834.517 L28.3562 828.629 L33.7671 828.629 Q30.5842 826.783 29.0564 823.982 Q27.4968 821.149 27.4968 817.234 Q27.4968 810.741 32.6531 806.699 Q37.8093 802.625 46.212 802.625 Q54.6147 802.625 59.771 806.699 Q64.9272 810.741 64.9272 817.234 Q64.9272 821.149 63.3994 823.982 Q61.8398 826.783 58.657 828.629 M46.212 808.704 Q39.7508 808.704 36.0905 811.378 Q32.3984 814.019 32.3984 818.666 Q32.3984 823.313 36.0905 825.987 Q39.7508 828.629 46.212 828.629 Q52.6732 828.629 56.3653 825.987 Q60.0256 823.313 60.0256 818.666 Q60.0256 814.019 56.3653 811.378 Q52.6732 808.704 46.212 808.704 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M67.3143 778.085 Q73.68 780.568 75.6216 782.923 Q77.5631 785.278 77.5631 789.225 L77.5631 793.904 L72.6615 793.904 L72.6615 790.466 Q72.6615 788.047 71.5157 786.711 Q70.3699 785.374 66.1048 783.75 L63.4312 782.7 L28.3562 797.118 L28.3562 790.912 L56.238 779.772 L28.3562 768.632 L28.3562 762.425 L67.3143 778.085 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M32.4621 719.807 Q32.4621 724.518 36.1542 727.255 Q39.8145 729.992 46.212 729.992 Q52.6095 729.992 56.3017 727.287 Q59.9619 724.549 59.9619 719.807 Q59.9619 715.128 56.2698 712.391 Q52.5777 709.654 46.212 709.654 Q39.8781 709.654 36.186 712.391 Q32.4621 715.128 32.4621 719.807 M27.4968 719.807 Q27.4968 712.168 32.4621 707.808 Q37.4273 703.447 46.212 703.447 Q54.9649 703.447 59.9619 707.808 Q64.9272 712.168 64.9272 719.807 Q64.9272 727.478 59.9619 731.838 Q54.9649 736.167 46.212 736.167 Q37.4273 736.167 32.4621 731.838 Q27.4968 727.478 27.4968 719.807 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M42.4881 664.107 L64.0042 664.107 L64.0042 669.963 L42.679 669.963 Q37.6183 669.963 35.1038 671.937 Q32.5894 673.91 32.5894 677.857 Q32.5894 682.599 35.6131 685.337 Q38.6368 688.074 43.8567 688.074 L64.0042 688.074 L64.0042 693.962 L28.3562 693.962 L28.3562 688.074 L33.8944 688.074 Q30.6797 685.973 29.0883 683.14 Q27.4968 680.276 27.4968 676.552 Q27.4968 670.409 31.3163 667.258 Q35.1038 664.107 42.4881 664.107 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M42.4881 602.073 L64.0042 602.073 L64.0042 607.93 L42.679 607.93 Q37.6183 607.93 35.1038 609.903 Q32.5894 611.876 32.5894 615.823 Q32.5894 620.566 35.6131 623.303 Q38.6368 626.04 43.8567 626.04 L64.0042 626.04 L64.0042 631.928 L14.479 631.928 L14.479 626.04 L33.8944 626.04 Q30.6797 623.939 29.0883 621.107 Q27.4968 618.242 27.4968 614.518 Q27.4968 608.375 31.3163 605.224 Q35.1038 602.073 42.4881 602.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M32.4621 576.579 Q32.4621 581.289 36.1542 584.026 Q39.8145 586.764 46.212 586.764 Q52.6095 586.764 56.3017 584.058 Q59.9619 581.321 59.9619 576.579 Q59.9619 571.9 56.2698 569.163 Q52.5777 566.425 46.212 566.425 Q39.8781 566.425 36.186 569.163 Q32.4621 571.9 32.4621 576.579 M27.4968 576.579 Q27.4968 568.94 32.4621 564.579 Q37.4273 560.219 46.212 560.219 Q54.9649 560.219 59.9619 564.579 Q64.9272 568.94 64.9272 576.579 Q64.9272 584.249 59.9619 588.61 Q54.9649 592.938 46.212 592.938 Q37.4273 592.938 32.4621 588.61 Q27.4968 584.249 27.4968 576.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M14.479 550.511 L14.479 544.655 L64.0042 544.655 L64.0042 550.511 L14.479 550.511 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M33.7671 508.943 L14.479 508.943 L14.479 503.086 L64.0042 503.086 L64.0042 508.943 L58.657 508.943 Q61.8398 510.789 63.3994 513.622 Q64.9272 516.423 64.9272 520.369 Q64.9272 526.831 59.771 530.905 Q54.6147 534.947 46.212 534.947 Q37.8093 534.947 32.6531 530.905 Q27.4968 526.831 27.4968 520.369 Q27.4968 516.423 29.0564 513.622 Q30.5842 510.789 33.7671 508.943 M46.212 528.899 Q52.6732 528.899 56.3653 526.258 Q60.0256 523.584 60.0256 518.937 Q60.0256 514.29 56.3653 511.617 Q52.6732 508.943 46.212 508.943 Q39.7508 508.943 36.0905 511.617 Q32.3984 514.29 32.3984 518.937 Q32.3984 523.584 36.0905 526.258 Q39.7508 528.899 46.212 528.899 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M32.4621 477.21 Q32.4621 481.921 36.1542 484.658 Q39.8145 487.395 46.212 487.395 Q52.6095 487.395 56.3017 484.69 Q59.9619 481.952 59.9619 477.21 Q59.9619 472.531 56.2698 469.794 Q52.5777 467.057 46.212 467.057 Q39.8781 467.057 36.186 469.794 Q32.4621 472.531 32.4621 477.21 M27.4968 477.21 Q27.4968 469.571 32.4621 465.211 Q37.4273 460.85 46.212 460.85 Q54.9649 460.85 59.9619 465.211 Q64.9272 469.571 64.9272 477.21 Q64.9272 484.881 59.9619 489.241 Q54.9649 493.57 46.212 493.57 Q37.4273 493.57 32.4621 489.241 Q27.4968 484.881 27.4968 477.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M49.9359 451.747 L28.3562 451.747 L28.3562 445.891 L49.7131 445.891 Q54.7739 445.891 57.3202 443.917 Q59.8346 441.944 59.8346 437.997 Q59.8346 433.255 56.8109 430.517 Q53.7872 427.748 48.5673 427.748 L28.3562 427.748 L28.3562 421.892 L64.0042 421.892 L64.0042 427.748 L58.5296 427.748 Q61.7762 429.881 63.3676 432.714 Q64.9272 435.515 64.9272 439.238 Q64.9272 445.381 61.1078 448.564 Q57.2883 451.747 49.9359 451.747 M27.4968 437.01 L27.4968 437.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M18.2347 404.036 L28.3562 404.036 L28.3562 391.973 L32.9077 391.973 L32.9077 404.036 L52.2594 404.036 Q56.6199 404.036 57.8613 402.858 Q59.1026 401.649 59.1026 397.989 L59.1026 391.973 L64.0042 391.973 L64.0042 397.989 Q64.0042 404.768 61.4897 407.346 Q58.9434 409.924 52.2594 409.924 L32.9077 409.924 L32.9077 414.221 L28.3562 414.221 L28.3562 409.924 L18.2347 409.924 L18.2347 404.036 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M29.4065 340.825 L34.9447 340.825 Q33.6716 343.307 33.035 345.981 Q32.3984 348.654 32.3984 351.519 Q32.3984 355.88 33.7352 358.076 Q35.072 360.24 37.7456 360.24 Q39.7826 360.24 40.9603 358.68 Q42.1061 357.121 43.1565 352.41 L43.6021 350.405 Q44.9389 344.167 47.3897 341.557 Q49.8086 338.915 54.1691 338.915 Q59.1344 338.915 62.0308 342.862 Q64.9272 346.777 64.9272 353.652 Q64.9272 356.516 64.3543 359.635 Q63.8132 362.723 62.6992 366.16 L56.6518 366.16 Q58.3387 362.914 59.198 359.763 Q60.0256 356.612 60.0256 353.524 Q60.0256 349.386 58.6251 347.158 Q57.1929 344.93 54.6147 344.93 Q52.2276 344.93 50.9545 346.554 Q49.6813 348.145 48.5037 353.588 L48.0262 355.625 Q46.8804 361.068 44.5251 363.487 Q42.138 365.905 38.0002 365.905 Q32.9713 365.905 30.2341 362.341 Q27.4968 358.776 27.4968 352.219 Q27.4968 348.973 27.9743 346.108 Q28.4517 343.244 29.4065 340.825 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M44.7161 299.097 L47.5806 299.097 L47.5806 326.024 Q53.6281 325.642 56.8109 322.396 Q59.9619 319.118 59.9619 313.293 Q59.9619 309.919 59.1344 306.768 Q58.3069 303.585 56.6518 300.466 L62.1899 300.466 Q63.5267 303.617 64.227 306.927 Q64.9272 310.237 64.9272 313.643 Q64.9272 322.173 59.9619 327.17 Q54.9967 332.135 46.5303 332.135 Q37.7774 332.135 32.6531 327.425 Q27.4968 322.682 27.4968 314.662 Q27.4968 307.468 32.1438 303.299 Q36.7589 299.097 44.7161 299.097 M42.9973 304.954 Q38.1912 305.018 35.3266 307.659 Q32.4621 310.269 32.4621 314.598 Q32.4621 319.499 35.2312 322.46 Q38.0002 325.388 43.0292 325.833 L42.9973 304.954 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M18.2347 283.692 L28.3562 283.692 L28.3562 271.629 L32.9077 271.629 L32.9077 283.692 L52.2594 283.692 Q56.6199 283.692 57.8613 282.515 Q59.1026 281.305 59.1026 277.645 L59.1026 271.629 L64.0042 271.629 L64.0042 277.645 Q64.0042 284.424 61.4897 287.003 Q58.9434 289.581 52.2594 289.581 L32.9077 289.581 L32.9077 293.878 L28.3562 293.878 L28.3562 289.581 L18.2347 289.581 L18.2347 283.692 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip342)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,86.1857 362.36,529.563 403.425,630.305 485.554,800.014 567.683,850.704 608.747,903.82 690.876,989.619 773.005,975.092 855.134,1003.07 896.198,1085.7 \n",
       "  978.327,1089.71 1060.46,1094.02 1101.52,1142.05 1183.65,1140.89 1265.78,1134.05 1306.84,1110.78 1388.97,1171.25 1471.1,1189.63 1512.17,1215.47 1594.29,1208.03 \n",
       "  1676.42,1248.73 1717.49,1211.66 1799.62,1223.07 1881.75,1231.57 1963.88,1261.05 2004.94,1317.44 2087.07,1241.46 2169.2,1325 2210.26,1290.95 2292.39,1384.24 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M1992.24 196.789 L2281.66 196.789 L2281.66 93.1086 L1992.24 93.1086  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1992.24,196.789 2281.66,196.789 2281.66,93.1086 1992.24,93.1086 1992.24,196.789 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2015.94,144.949 2158.13,144.949 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M2195.67 164.636 Q2193.87 169.266 2192.16 170.678 Q2190.44 172.09 2187.57 172.09 L2184.17 172.09 L2184.17 168.525 L2186.67 168.525 Q2188.43 168.525 2189.4 167.692 Q2190.37 166.858 2191.55 163.756 L2192.32 161.812 L2181.83 136.303 L2186.35 136.303 L2194.45 156.581 L2202.55 136.303 L2207.06 136.303 L2195.67 164.636 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2214.35 158.293 L2221.99 158.293 L2221.99 131.928 L2213.68 133.595 L2213.68 129.335 L2221.95 127.669 L2226.62 127.669 L2226.62 158.293 L2234.26 158.293 L2234.26 162.229 L2214.35 162.229 L2214.35 158.293 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(\"epochs\")\n",
    "ylabel!(\"cross entropy on holdout set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: \n",
    "\n",
    "- `evaluate!()` let's you see the results of a model evaluated with different resampling schemes against desired metrics\n",
    "- `evaluate()` does not fit the model \n",
    "- First use evaluate to see if fitting will work, then fit the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
